{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import matplotlib as plt\n",
    "import cv2\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam1 enabled\n",
      "cam2 enabled\n",
      "cams configued\n"
     ]
    }
   ],
   "source": [
    "ctx = rs.context()\n",
    "devices = ctx.query_devices()\n",
    "var = [i.get_info(rs.camera_info.serial_number) for i in devices]\n",
    "\n",
    "#set up master and slave\n",
    "dev = devices[0].first_depth_sensor()\n",
    "dev.set_option(rs.option.inter_cam_sync_mode,1)\n",
    "dev.get_option(rs.option.inter_cam_sync_mode)\n",
    "\n",
    "dev2 = devices[1].first_depth_sensor()\n",
    "dev2.set_option(rs.option.inter_cam_sync_mode,2)\n",
    "dev2.get_option(rs.option.inter_cam_sync_mode)\n",
    "\n",
    "#start pipeline\n",
    "#cam1: configure depth and color streams\n",
    "pipeline1 = rs.pipeline()\n",
    "config1 = rs.config()\n",
    "config1.enable_device(var[0])\n",
    "config1.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 90)\n",
    "config1.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 60)\n",
    "#config1.enable_record_to_file('master.bag')\n",
    "print('cam1 enabled')\n",
    "\n",
    "#cam2: configure depth and color streams \n",
    "pipeline2 = rs.pipeline()\n",
    "config2 = rs.config()\n",
    "config2.enable_device(var[1])\n",
    "config2.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 90)\n",
    "config2.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 60)\n",
    "#config2.enable_record_to_file('slave.bag')\n",
    "print('cam2 enabled')\n",
    "prof = pipeline1.start(config1)\n",
    "prof2 = pipeline2.start(config2)\n",
    "print('cams configued')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take checkerboard images\n",
    "try:\n",
    "    start = time.time()\n",
    "    mastercount = []\n",
    "    slavecount = []\n",
    "    d = 1\n",
    "    while time.time() - start <100:\n",
    "\n",
    "        #camera 1\n",
    "        frames1 = pipeline1.wait_for_frames()\n",
    "        depthframe1 = frames1.get_depth_frame()\n",
    "        colframe1 = frames1.get_color_frame()\n",
    "        if not depthframe1 or not colframe1:\n",
    "            continue\n",
    "\n",
    "\n",
    "        #convert images to numpy arrays\n",
    "        depthim1 = np.asanyarray(depthframe1.get_data())\n",
    "        colim1 = np.asanyarray(colframe1.get_data())\n",
    "\n",
    "        #apply color map on depth image, convert to 8bit pixel\n",
    "        depthcolmap1 = cv2.applyColorMap(cv2.convertScaleAbs(depthim1, alpha=0.5), cv2.COLORMAP_JET)\n",
    "\n",
    "        #camera 2\n",
    "        frames2 = pipeline2.wait_for_frames()\n",
    "        depthframe2 = frames2.get_depth_frame()\n",
    "        colframe2 = frames2.get_color_frame()\n",
    "        if not depthframe2 or not colframe2:\n",
    "            continue\n",
    "\n",
    "\n",
    "        #check time sync\n",
    "        #print([depthframe1, depthframe2]) #returns system_time\n",
    "        #counter = depthframe1.get_frame_metadata(rs.frame_metadata_value.frame_counter)\n",
    "        #mastercount.append(counter)\n",
    "        #counter2 = depthframe2.get_frame_metadata(rs.frame_metadata_value.frame_counter)\n",
    "        #slavecount.append(counter2)\n",
    "        #timestamp1 = depthframe1.get_timestamp()\n",
    "        #timestamp2 = depthframe2.get_timestamp()\n",
    "        #print([counter,counter2, timestamp1, timestamp2])\n",
    "        #convert images to numpy arrays\n",
    "        depthim2 = np.asanyarray(depthframe2.get_data())\n",
    "        colim2 = np.asanyarray(colframe2.get_data())\n",
    "\n",
    "        #apply color map on depth image, convert to 8bit pixel\n",
    "        depthcolmap2 = cv2.applyColorMap(cv2.convertScaleAbs(depthim2, alpha=0.5), cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "        #stack all images horizontally\n",
    "        images = np.hstack((colim1, depthcolmap1, colim2, depthcolmap2))\n",
    "\n",
    "        #show images from cameras\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        #save images and depth map from both cameras by pressing \"s\"\n",
    "        ch = cv2.waitKey(25)\n",
    "        if ch==115:\n",
    "            filename = \"checkerboard/master_%d.jpg\"%d\n",
    "            filename2 = \"checkerboard/slave_%d.jpg\"%d\n",
    "            cv2.imwrite(filename, colim1)\n",
    "            cv2.imwrite(filename2, colim2)\n",
    "            d+=1\n",
    "            #cv2.imwrite(\"depth1.jpg\", depthcolmap1)\n",
    "            #cv2.imwrite(\"depth2.jpg\", depthcolmap2)\n",
    "\n",
    "finally:\n",
    "    #stop streaming\n",
    "    pipeline1.stop()\n",
    "    pipeline2.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    del pipeline1\n",
    "    del config1\n",
    "    del pipeline2\n",
    "    del config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate extrinsic and intrinsic params from above checkerboards\n",
    "#currently for one camera\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "#prepare object points\n",
    "objp = np.zeros((7*9,3),np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:9].T.reshape(-1,2)\n",
    "\n",
    "#arrays to store object points and image points from all images\n",
    "objpoints = []\n",
    "imgpoints = []\n",
    "\n",
    "path = '/home/alissa/IntelRealSense/checkerboard/'\n",
    "images = glob.glob(path + '/master_*.jpg')\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #find chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (7,9),None)\n",
    "    #if found add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11),(-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        #draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (7,9), corners2, ret)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(500)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns camera matrix, distortion coefficients, rotation, and translation vectors\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument '%s'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0e2d883c14bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get extrinsics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvePnP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument '%s'"
     ]
    }
   ],
   "source": [
    "#get extrinsics\n",
    "cv2.solvePnP(objpoints, corners, mtx, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera matrix [[532.72339941   0.         466.42753055]\n",
      " [  0.         510.36751616 243.05093151]\n",
      " [  0.           0.           1.        ]]\n",
      "distortion coeff [[-0.16851522  1.30677432  0.01250941  0.07521741 -4.6680607 ]]\n",
      "rotation [array([[-0.14758784],\n",
      "       [ 0.258183  ],\n",
      "       [-0.93077796]]), array([[-0.16652445],\n",
      "       [ 0.18601531],\n",
      "       [-0.20942054]]), array([[-0.10598097],\n",
      "       [ 0.32237389],\n",
      "       [-0.28935504]]), array([[0.43968074],\n",
      "       [0.83892158],\n",
      "       [1.55071622]]), array([[-0.32789176],\n",
      "       [ 0.21257754],\n",
      "       [-0.84488081]]), array([[0.1302787 ],\n",
      "       [0.17720107],\n",
      "       [1.05794602]]), array([[0.10906699],\n",
      "       [0.21261334],\n",
      "       [1.15867393]]), array([[0.07104742],\n",
      "       [0.31345124],\n",
      "       [0.61195049]]), array([[0.04475727],\n",
      "       [0.20319984],\n",
      "       [1.09208668]]), array([[0.70246517],\n",
      "       [0.7135817 ],\n",
      "       [1.36043029]]), array([[-0.16971438],\n",
      "       [-0.17936405],\n",
      "       [ 1.13941748]]), array([[-0.08312169],\n",
      "       [-0.28403334],\n",
      "       [ 0.96818601]])]\n",
      "translation [array([[-6.07827155],\n",
      "       [-0.63131003],\n",
      "       [20.81220796]]), array([[-5.08570582],\n",
      "       [-3.03892372],\n",
      "       [20.06963802]]), array([[-5.31123229],\n",
      "       [-0.42810007],\n",
      "       [20.4562    ]]), array([[ 3.68145353],\n",
      "       [-1.85894416],\n",
      "       [17.80797963]]), array([[-7.16194477],\n",
      "       [ 2.50010627],\n",
      "       [22.77423473]]), array([[-1.72836205],\n",
      "       [-4.54115381],\n",
      "       [18.62234155]]), array([[ 1.974568  ],\n",
      "       [-5.09381593],\n",
      "       [18.90295236]]), array([[-1.78047576],\n",
      "       [-4.52433277],\n",
      "       [17.1094133 ]]), array([[-0.15053287],\n",
      "       [-3.57204095],\n",
      "       [16.72391243]]), array([[ 1.82762141],\n",
      "       [-3.71621735],\n",
      "       [17.84291983]]), array([[ 0.25517779],\n",
      "       [-4.57950364],\n",
      "       [22.52261962]]), array([[ 0.27258982],\n",
      "       [-5.39242685],\n",
      "       [19.86837699]])]\n"
     ]
    }
   ],
   "source": [
    "print('camera matrix', mtx)\n",
    "print('distortion coeff', dist)\n",
    "print('rotation', rvecs)\n",
    "print('translation', tvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "get_active_profile() can only be called between a start() and a following stop()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1665ffeaf79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_active_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: get_active_profile() can only be called between a start() and a following stop()"
     ]
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "pipeline.cong\n",
    "profile = pipeline.get_active_profile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
